---
title: "Extreme Value Statistics Draft 1"
Dataset: Wind 
Group: 1
Name: Chaitanya Arora, Diandra Gramma 
output: html_document
---

## Introduction ##

Before deep diving into the analysis and coding portion,we should analyse the type of data that we have and what does it mean. 

The wind dataset is composed of two files:

1. The file named "coordinates": it contains the coordinates of 5 places in north- western Europe, coordinates which we will analyse during the process of the project;

2. The file named "dataset": is a large matrix containing the values of the maximum wind-speed which lasted more than 3 seconds over a time period of 3 hours;


The information found above has been collected over a period of 35 years, more specifically from 10th January 1979 to 4th January 2014.


## Step 1: Basic Analysis of the data ##

1.1 The graphical representation of the 5 places we are analysing in this project

```{r}

load("Datasets/1_Wind.RData")
library(ggmap)

bbox <- c(-21.375,  40.875,  16.875,  61.125)

map <- get_map(location = bbox, source="stamen", maptype = "toner" , color="bw")

dataToPlot = data.frame(X = coordinates[, 1], Y = coordinates[, 2])
mapPoints <- ggmap(map) +
  geom_point(
    data = dataToPlot,
    aes(x = X, y = Y),
    color = "red",
    alpha = 1,
    size = 3,
    shape = 10,
  ) +
  labs(title = "", fill = "") +
  theme(plot.title = element_text(hjust = 0.5)) + theme(legend.position = "none") +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text=element_text(size=14))
print(mapPoints)


```
As it can be seen from the graphic above, the 5 places contained in this project can be found in: Dublin, United Kingdom and lastly, the North See. Throughout this report the great saying of Edward Davey "Britain has a lot of wind. It's our wind. We don't have to import it", will be confirmed, as well as a in-depth analysis on how the speed, location and time contribute to it.

1.2 The Basic plots to understand the graph better

The first graphic that will be presented is the histogram. This histogram describes a way of identifying how the speeds of the winds with respect to time influence the frequency of such events happening in all 5 locations. 

Therefore, we will have the 5 plots:


```{r}
library(lattice)
data_numeric_v1 <- as.numeric(dataset[,1])
data_numeric_v2 <- as.numeric(dataset[,2])
data_numeric_v3 <- as.numeric(dataset[,3])
data_numeric_v4 <- as.numeric(dataset[,4])
date_numeric_v5 <- as.numeric(dataset[,5])
hist(data_numeric_v1,breaks=30,main="Frequency of wind speeds in the Area v1",xlab = "winds speed in V1")

hist(data_numeric_v2,breaks=30,main="Frequency of wind speeds in the Area v2",xlab = "winds speed in V2")

hist(data_numeric_v3,breaks=30,main="Frequency of wind speeds in the Area v3",xlab = "winds speed in V3")

hist(data_numeric_v4,breaks=30,main="Frequency of wind speeds in the Area v4",xlab = "winds speed in V4")

hist(data_numeric_v4,breaks=30,main="Frequency of wind speeds in the Area v5",xlab = "winds speed in V5")

```

Conclusion:

Despite the fact that the histograms have quite a similar output, they give a rigorous result on the most frequent winds found in the 5 areas. 

It can be seen the fact that the most frequent speeds occur between 10-20 km/h, more specifically at about 13/14 km/h. However the figure for the 3rd location seems to represent the fact that for this area, the speed of the wind is frequent at low levels, which only means the fact that this specific point is located in a region with decreased winds. 

Another interesting point to be taking into consideration is the fact that the last 2 location have the same exact graphic. (???????? Question, where the hell are these loc and which one is which???????)

The second graphic will be the Empirical distribution one. It will focuse, as the one presented above, on how the speeds of the winds with respect to time influence the frequency of extreme event happening in all of the 5 locations given.




```{r}
dates_new=as.Date(dates)
xyplot(dataset[,1]~dates_new,pch=1,ylab=" ")
xyplot(dataset[,2]~dates_new,pch=1,ylab=" ")
xyplot(dataset[,3]~dates_new,pch=1,ylab=" ")
xyplot(dataset[,4]~dates_new,pch=1,ylab=" ")
xyplot(dataset[,5]~dates_new,pch=1,ylab=" ")

```



<< analysis>>



```{r}
dates_new=as.Date(dates)
yr=factor(year(dates_new),levels=c("1979","1980","1981","1982","1983","1984","1985","1986","1987","1988","1989","1990","1991","1992","1993","1994","1995","1996","1997","1998","1999","2000","2001","2002","2003","2004","2005","2006","2007","2008","2009","2010","2011","2012","2013","2014"),ordered=TRUE)
bwplot(yr~dataset[,1])
bwplot(yr~dataset[,2])
bwplot(yr~dataset[,3])
bwplot(yr~dataset[,4])
bwplot(yr~dataset[,5])

```
<<< analysis >>


```{r}
dotplot(dataset[,1],pch=1)
dotplot(dataset[,2],pch=1)
dotplot(dataset[,3],pch=1)
dotplot(dataset[,4],pch=1)
dotplot(dataset[,5],pch=1)


```
<<< analysis>>

```{r}
qqmath(dataset[,1])
qqmath(dataset[,2])
qqmath(dataset[,3])
qqmath(dataset[,4])
qqmath(dataset[,5])
```
<< analysis>>


```{r}
qqmath(dataset[,1],distri=qexp)
qqmath(dataset[,2],distri=qexp)
qqmath(dataset[,3],distri=qexp)
qqmath(dataset[,4],distri=qexp)
qqmath(dataset[,5],distri=qexp)
```
<< analysis>>

```{r}
var1 <- gev.fit(dataset[,1])
# Xi = -0.135 --> Weibull
m <- gev.diag(var1)
```

```{r}
var2 <- gev.fit(dataset[,2])
# Xi = -0.138 --> Weibull 
n <- gev.diag(var2)

```

```{r}
var3 <- gev.fit(dataset[,3])
# Xi = -0.086 --> Gumbell
o <- gev.diag(var3)

```

```{r}
var4 <- gev.fit(dataset[,4])
# Xi = -0.123 --> Weibull
p <- gev.diag(var4)
```


```{r}
var5 <- gev.fit(dataset[,5])
# Xi = -0.129 --> weibull
q <- gev.diag(var5)
```

Doubt: Not able to check the profile liklihood 
since X value is -0.13 it follows this distrubution
calculate x n tell distrbution


mean excess plot left

```{r}
mrl.plot(dataset[,1])
mrl.plot(dataset[,2])
mrl.plot(dataset[,3])
mrl.plot(dataset[,4])
mrl.plot(dataset[,5])
```

<<<<<<<<<<<<<<<<<< Univariate Analysis >>>>>>>>>>>>>>>>>>


ANCOVA Analysis (analysis of covariance)

```{r}
summary(aov(dataset[,1]~dataset[,2]))
```

Scatterplot

Line of best fit

Regression Analysis 

whatever else u find relevant


<<<<<<<<< Multivariate analysis >>>>>

we start with scatterplot 

```{r}
mydata=dataset[,-1]
pairs(mydata)
```

 Marginal standardization based on the rank transform

```{r}
n=nrow(mydata)
myranks=apply(mydata,2,rank)
data.unif=myranks/(n+1)
data.stand=1/(1-data.unif)
```

Estimation of the extremal coefficient

```{r}
p=0.8+0:19*0.01
u.star=1/(1-p)
u.star[1]*mean(apply(data.stand>u.star[1],1,any))
u.star[20]*mean(apply(data.stand>u.star[20],1,any))
theta.est=c()
for(i in 1:length(p)){
  theta.est[i]=u.star[i]*mean(apply(data.stand>u.star[i],1,any))
}
plot(p,theta.est,type="l",lwd=3,xlab="p",ylab=expression(theta[5]))
# calculate approximate 95% pointwise confidence intervals
sd_est = sqrt(u.star/n * theta.est *(1-theta.est/u.star))
lines(p,theta.est + qnorm(.975)*sd_est, col = "blue", lwd = 3, lty = 2)
lines(p,theta.est - qnorm(.975)*sd_est, col = "blue", lwd = 3, lty = 2)
```
Estimation of the coefficient of tail dependence

```{r}
library(fExtremes)
hillPlot(apply(data.stand,1,min), start=0.8*n, plottype="xi") 
```

Modeling extremal dependence 
 
We first construct the pseudo-polar coordinates and then extract the angles w corresponding to extreme events. 


```{r}
r.obs=apply(data.stand,1,sum)
w.obs=data.stand/r.obs
r_0=quantile(r.obs,0.95)
w.H=w.obs[r.obs>r_0,]
```
 
### 3.2) Estimation of joint exceedances above the overall 95% quantile

```{r}
q95=quantile(mydata,0.95)
q95
prob.component=apply(mydata<q95,2,mean)
u.star=1/(1-prob.component)
u.star
ell.min=function(w,u){min(w/u)} #already defined in preceding exercice
d=5
n_0=nrow(w.H)
#estimate the probability of the joint exceedance event:
d/n_0*sum(apply(w.H,1,ell.min,u=u.star))
#compare to observed empirical probability:
mean(apply(t(t(data.stand)/u.star)>1,1,all))

```

Estimation of a parametric Dirichlet model for the angular measure


```{r}
library(MCMCpack) #for Dirichlet density 
library(maxLik) #for numerical maximisation of the log-likelihood
```

```{r}

#define the log-likelihood function:
llfun=function(alpha){
if(alpha<=0) return(-Inf) #inadmissible parameters
 sum(log(ddirichlet(w.H,alpha=rep(alpha,d))))
}

initpar=1 # define the initial value of the parameter
fit=maxLik(llfun,start=initpar)
summary(fit)

```